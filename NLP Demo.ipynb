{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Machine Learning and NLP</u></h2>\n",
    "\n",
    "# Module 5 - Natural Language Processing\n",
    "\n",
    "<h2>In Class Codes</h2>\n",
    "\n",
    "In this demo, you will be shown how to perform various techniques learnt throughout this module using libraries from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Types Of Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "s = \"Hello! Let's use NLTK.\"\n",
    "print(word_tokenize(s))\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "print(wordpunct_tokenize(s))\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "print(sent_tokenize(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Tokens Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import bigrams, trigrams, ngrams\n",
    "#let us consider the below string for the example\n",
    "string = \"ML can be seen as a time-saving device that allows humans to explore their more creative ambitions while ML is in the background crunching numbers\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_tokens=nltk.word_tokenize(string)\n",
    "ML_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Creating Bigrams And Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_bigrams=list(nltk.bigrams(ML_tokens))\n",
    "ML_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ML_trigrams=list(nltk.trigrams (ML_tokens))\n",
    "ML_trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### POS Tagging - Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in ML_tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Shortcomings Of POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent= \"Jim eats a banana\"\n",
    "tokens = word_tokenize(sent)\n",
    "for token in tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tokenizer = RegexpTokenizer('(?u)\\W+|\\$[\\d\\.]+|\\S+')\n",
    "regextokens = reg_tokenizer.tokenize(sent)\n",
    "regextags = nltk.pos_tag(regextokens)\n",
    "regextags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words =stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Removing Stop Words Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence = [w for w in ML_tokens if not w in stop_words] \n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming Using NLTK – Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst=PorterStemmer()\n",
    "print(pst.stem(\"Measure\"))\n",
    "print(pst.stem(\"Measurement\"))\n",
    "print(pst.stem(\"Measuring\"))\n",
    "print(pst.stem(\"Measurer\"))\n",
    "print(pst.stem(\"Measures\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Non-English Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sbst=SnowballStemmer(\"spanish\")\n",
    "print(sbst.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sbst.stem(\"producción\"))\n",
    "print(sbst.stem(\"producto\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization Using NLTK - Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem = WordNetLemmatizer()\n",
    "print(word_lem.lemmatize(\"eating\", pos=\"v\"))\n",
    "print(word_lem.lemmatize(\"eats\", pos=\"v\"))\n",
    "print(word_lem.lemmatize(\"ate\", pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "print(\"Result of WordNetLemmatizer: \", word_lem.lemmatize(\"gone\", pos=\"v\"))\n",
    "\n",
    "print(\"Result of PorterStemmer: \", PorterStemmer().stem(\"gone\"))\n",
    "\n",
    "print(\"Result of LancasterStemmer: \", LancasterStemmer().stem(\"gone\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NER Using NLTK - Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import ne_chunk\n",
    "\n",
    "ex = \"Ferrari is an Italian luxury sports car manufacturer based in Maranello. Founded by Enzo Ferrari in 1939, the company built its first car in 1940.\"\n",
    "\n",
    "tokenized = nltk.word_tokenize(ex)\n",
    "tagged = nltk.pos_tag(tokenized)\n",
    "namedEnt = ne_chunk(tagged)\n",
    "    \n",
    "print(namedEnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### WSD Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sent1 = \"She is looking for a match\"\n",
    "sent2 = \"Yesterday's Football match was exciting\"\n",
    "\n",
    "print(lesk(word_tokenize(sent1), 'match'))\n",
    "print(lesk(word_tokenize(sent2), 'match'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TF-IDF Using Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1 = 'The movie was good and we really like it'\n",
    "review_2 = 'the movie was good but the ending was boring'\n",
    "review_3 = 'we did not like the movie as it was too lengthy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vect = TfidfVectorizer(min_df=1,lowercase=True,stop_words='english')\n",
    "review_1 = 'The movie was good and we really like it'\n",
    "review_2 = 'the movie was good but the ending was boring'\n",
    "review_3 = 'we did not like the movie as it was too lengthy'\n",
    "review_list = [review_1,review_2,review_3]\n",
    "\n",
    "tf_matrix = tf_vect.fit_transform(review_list)\n",
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tf_names = tf_vect.get_feature_names()\n",
    "tf_df = pd.DataFrame(tf_matrix.toarray(),columns=tf_names)\n",
    "tf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TextBlob For Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "print(TextBlob('great').sentiment)\n",
    "print(TextBlob('dark').sentiment)\n",
    "print(TextBlob('excellent').sentiment)\n",
    "print(TextBlob('boring').sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b><i>Conclusion</i></b>: In this demo, we examined how to apply various text pre-processing techniques using NLTK and TextBlob."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
