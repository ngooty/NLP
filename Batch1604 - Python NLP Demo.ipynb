{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,wordpunct_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tokens uinsg NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Hello!, am learning Natural language processing from today. This is part of my Data Science Training. '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', ',', 'am', 'learning', 'Natural', 'language', 'processing', 'from', 'today', '.', 'This', 'is', 'part', 'of', 'my', 'Data', 'Science', 'Training', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!,', 'am', 'learning', 'Natural', 'language', 'processing', 'from', 'today', '.', 'This', 'is', 'part', 'of', 'my', 'Data', 'Science', 'Training', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello!, am learning Natural language processing from today.', 'This is part of my Data Science Training.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regular expressions \n",
    "#sub, find/replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams,trigrams, ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens=word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', '!'),\n",
       " ('!', ','),\n",
       " (',', 'am'),\n",
       " ('am', 'learning'),\n",
       " ('learning', 'Natural'),\n",
       " ('Natural', 'language'),\n",
       " ('language', 'processing'),\n",
       " ('processing', 'from'),\n",
       " ('from', 'today'),\n",
       " ('today', '.'),\n",
       " ('.', 'This'),\n",
       " ('This', 'is'),\n",
       " ('is', 'part'),\n",
       " ('part', 'of'),\n",
       " ('of', 'my'),\n",
       " ('my', 'Data'),\n",
       " ('Data', 'Science'),\n",
       " ('Science', 'Training'),\n",
       " ('Training', '.')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_bigrams=list(nltk.bigrams(text_tokens))\n",
    "text_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', '!', ','),\n",
       " ('!', ',', 'am'),\n",
       " (',', 'am', 'learning'),\n",
       " ('am', 'learning', 'Natural'),\n",
       " ('learning', 'Natural', 'language'),\n",
       " ('Natural', 'language', 'processing'),\n",
       " ('language', 'processing', 'from'),\n",
       " ('processing', 'from', 'today'),\n",
       " ('from', 'today', '.'),\n",
       " ('today', '.', 'This'),\n",
       " ('.', 'This', 'is'),\n",
       " ('This', 'is', 'part'),\n",
       " ('is', 'part', 'of'),\n",
       " ('part', 'of', 'my'),\n",
       " ('of', 'my', 'Data'),\n",
       " ('my', 'Data', 'Science'),\n",
       " ('Data', 'Science', 'Training'),\n",
       " ('Science', 'Training', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_trigrams=list(nltk.trigrams(text_tokens))\n",
    "text_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NOUN')]\n",
      "[('!', '.')]\n",
      "[(',', '.')]\n",
      "[('am', 'VERB')]\n",
      "[('learning', 'VERB')]\n",
      "[('Natural', 'ADJ')]\n",
      "[('language', 'NOUN')]\n",
      "[('processing', 'NOUN')]\n",
      "[('from', 'ADP')]\n",
      "[('today', 'NOUN')]\n",
      "[('.', '.')]\n",
      "[('This', 'DET')]\n",
      "[('is', 'VERB')]\n",
      "[('part', 'NOUN')]\n",
      "[('of', 'ADP')]\n",
      "[('my', 'PRON')]\n",
      "[('Data', 'NOUN')]\n",
      "[('Science', 'NOUN')]\n",
      "[('Training', 'NOUN')]\n",
      "[('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for token in text_tokens:\n",
    "    print(nltk.pos_tag([token],tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('jack', 'NN')]\n",
      "[('eats', 'NNS')]\n",
      "[('a', 'DT')]\n",
      "[('banana', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "string='jack    eats   a banana'\n",
    "#string='things matter in life'\n",
    "\n",
    "tokens=word_tokenize(string)\n",
    "for token in tokens:\n",
    "    print(nltk.pos_tag([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-a1df784c0869>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-a1df784c0869>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    my laptop costs $1300.45  \\$ - \\./python - cwd/python nlp_demo123.this\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "my laptop costs $1300.45  \\$ - \\./python - cwd/python nlp_demo123.this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "reg_tokenizer=RegexpTokenizer('\\W+|\\$[\\d\\.]+|\\S+')\n",
    "regtokens=reg_tokenizer.tokenize(string)\n",
    "regtags=nltk.pos_tag(regtokens)\n",
    "regtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stopwords - to, is, am, with\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence=[w for w in text_tokens if not w in stop_words ]\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you got the text\n",
    "#text pre processing - we are here\n",
    "    tokenization\n",
    "    n-grams\n",
    "    postag\n",
    "    removing stop words\n",
    "    stemming/lemmatization\n",
    "    NER\n",
    "    WSD\n",
    "# \n",
    "#text vectorization - \n",
    "# apply ML on text vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#processing - process  - processed  - processor - \n",
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "pst=PorterStemmer()\n",
    "pst=LancasterStemmer()\n",
    "pst=SnowballStemmer('english')\n",
    "print(pst.stem('Training'))\n",
    "print(pst.stem('Train'))\n",
    "print(pst.stem('Trained'))\n",
    "print(pst.stem('Trainer'))\n",
    "print(pst.stem('Trainee'))\n",
    "print(pst.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatization - lemma - originial form \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#go, went, gone - go - \n",
    "#ride, rode, ridden\n",
    "word_lem=WordNetLemmatizer()\n",
    "print(word_lem.lemmatize('go',pos='v'))\n",
    "print(word_lem.lemmatize('went',pos='v'))\n",
    "print(word_lem.lemmatize('gone',pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "pst=PorterStemmer()\n",
    "pst=LancasterStemmer()\n",
    "pst=SnowballStemmer('english')\n",
    "print(pst.stem('go'))\n",
    "print(pst.stem('went'))\n",
    "print(pst.stem('gone'))\n",
    "print(pst.stem('stroke'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "#go, went, gone - go - \n",
    "#ride, rode, ridden\n",
    "word_lem=WordNetLemmatizer()\n",
    "print(word_lem.lemmatize('training',pos='v'))\n",
    "print(word_lem.lemmatize('trainer',pos='n'))\n",
    "print(word_lem.lemmatize('trainee',pos='n'))\n",
    "print(word_lem.lemmatize('trained',pos='v'))\n",
    "print(word_lem.lemmatize('training',pos='v'))\n",
    "print(word_lem.lemmatize('trainer',pos='n'))\n",
    "print(word_lem.lemmatize('trainee',pos='n'))\n",
    "print(word_lem.lemmatize('stroke',pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Named Entity Recognition - \n",
    "string='Bind Madhav started Machintosh in California in 1974. He had also started Pixel which was eventually sold out to Disney.'\n",
    "from nltk import ne_chunk\n",
    "tokenized_str=nltk.word_tokenize(string)\n",
    "tagged=nltk.pos_tag(tokenized_str)\n",
    "namedent=ne_chunk(tagged)\n",
    "print(namedent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''Machine learning is ....\n",
    "...supervised.....,\n",
    "classifcation,...\n",
    "'''\n",
    "for x in text:\n",
    "    if x contains 'classifcation':\n",
    "        value= \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word sense disambiguity\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "sent1='She was looking for a match'\n",
    "sent2=\"yesterday's football match was exciting\"\n",
    "print(lesk(word_tokenize(sent1),'match'))\n",
    "print(lesk(word_tokenize(sent2),'match'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "synset=wordnet.synsets('score')\n",
    "print(synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset[4].name()\n",
    "synset[4].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synset[6].name()\n",
    "synset[6].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pos, token, wsd, ner\n",
    "#use these results to form vectors/covert it to numbers \n",
    "# sentiment analysis\n",
    "# translations\n",
    "# responses/bots\n",
    "# numbers - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words\n",
    "#TF/IDF "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1='The movie was good and we really like it'\n",
    "review_2='The movie was good but the ending was boring'\n",
    "review_3='we did not like the movie as it was too lengthy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_vect=TfidfVectorizer(min_df=1,lowercase=True,stop_words='english')\n",
    "review=[review_1,review_2,review_3]\n",
    "tf_matrix=tf_vect.fit_transform(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 9)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x9 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 12 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "tf_names=tf_vect.get_feature_names()\n",
    "tf_df=pd.DataFrame(tf_matrix.toarray(),columns=tf_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>boring</th>\n",
       "      <th>did</th>\n",
       "      <th>ending</th>\n",
       "      <th>good</th>\n",
       "      <th>lengthy</th>\n",
       "      <th>like</th>\n",
       "      <th>movie</th>\n",
       "      <th>movive</th>\n",
       "      <th>really</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.562829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.562829</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.428046</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     boring       did    ending      good   lengthy      like     movie  \\\n",
       "0  0.000000  0.000000  0.000000  0.428046  0.000000  0.428046  0.000000   \n",
       "1  0.562829  0.000000  0.562829  0.428046  0.000000  0.000000  0.428046   \n",
       "2  0.000000  0.562829  0.000000  0.000000  0.562829  0.428046  0.428046   \n",
       "\n",
       "     movive    really  \n",
       "0  0.562829  0.562829  \n",
       "1  0.000000  0.000000  \n",
       "2  0.000000  0.000000  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_1='The movie was good and we really like it'\n",
    "review_2='The movie was good but the ending was boring'\n",
    "review_3='we did not like the movie as it was too lengthy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polarity - -1 to +1 - range; -1 is -ve Sentiment, 0 - Neutral Sentiment - +1 is +ve sentiment\n",
    "#Subjectivty - 0 to 1 - 0 is most subjective, 1 is the most objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=0.8, subjectivity=0.75)\n"
     ]
    }
   ],
   "source": [
    "print(TextBlob('great').sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment(polarity=-1.0, subjectivity=1.0)\n"
     ]
    }
   ],
   "source": [
    "print(TextBlob('boring').sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Machine Learning - rows/columns - dataframe - \n",
    "#association rules\n",
    "# recommender systems\n",
    "# text format - convert to numbers in row/columns format  - \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
